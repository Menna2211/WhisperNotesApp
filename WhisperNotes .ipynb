{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7961fe90"
      },
      "source": [
        "### Important Libraries\n",
        "\n",
        "This notebook utilizes the following key libraries:\n",
        "\n",
        "*   `os`: For interacting with the operating system, particularly for environment variables.\n",
        "*   `gradio`: To create interactive web interfaces for machine learning models.\n",
        "*   `whisper`: OpenAI's robust speech-to-text model for transcription.\n",
        "*   `openai`: The official Python client for the OpenAI API, used here to interact with OpenRouter.\n",
        "*   `pathlib`: For object-oriented filesystem paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "789aed5f",
        "outputId": "62e99a0b-d7c6-4ae0-f749-48f76b35532e"
      },
      "source": [
        "!pip install -q openai-whisper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe467932"
      },
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import whisper\n",
        "import openai\n",
        "from pathlib import Path\n",
        "from openai import OpenAI # Import the OpenAI client class"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc1b3dc"
      },
      "source": [
        "### Configure OpenRouter API\n",
        "\n",
        "This section sets up the OpenRouter API for accessing various LLMs. Make sure to replace `<YOUR_OPENROUTER_KEY>` with your actual API key. You can also use Colab's secrets manager for more secure handling of your API key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77c80cfc"
      },
      "source": [
        "# Configure OpenRouter\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=userdata.get('OPENROUTER_API_KEY'),\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2f5360"
      },
      "source": [
        "### Load Whisper Model\n",
        "\n",
        "This loads the `base` Whisper model for speech-to-text transcription. Depending on your needs and available resources, you might choose a larger model (e.g., `small`, `medium`, `large`) for better accuracy, or a smaller one (e.g., `tiny`) for faster inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b804a21b",
        "outputId": "c396c5f7-def9-40d7-fb5a-1b1a47d57fdb"
      },
      "source": [
        "model = whisper.load_model(\"base\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 110MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdf1b4ad"
      },
      "source": [
        "### Define Transcription and Summarization Function\n",
        "\n",
        "This Python function `transcribe_and_summarize_or_translate` takes an audio file, transcribes it using Whisper, and optionally summarizes the transcript using an LLM via OpenRouter. It also detects the language and prepares a downloadable text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90bb73a7"
      },
      "source": [
        "def transcribe_and_summarize_or_translate(audio, summarize, target_language=None):\n",
        "    # Step 1: Whisper transcription\n",
        "    result = model.transcribe(audio)\n",
        "    transcript = result['text']\n",
        "    detected_language = result.get('language', 'unknown')\n",
        "\n",
        "    summary = None\n",
        "    if summarize:\n",
        "        # Example: Summarization via LLM (through OpenRouter) using the new client\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b:free\",  # or other model available on OpenRouter\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Please summarize the following text:\\n{transcript}\"}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=400\n",
        "        )\n",
        "        summary = resp.choices[0].message.content\n",
        "\n",
        "    # Build download text\n",
        "    output_text = transcript\n",
        "    if summary:\n",
        "        output_text += \"\\n\\n--- Summary ---\\n\" + summary\n",
        "\n",
        "    download_path = Path(\"transcript.txt\")\n",
        "    download_path.write_text(output_text, encoding=\"utf-8\")\n",
        "\n",
        "    return transcript, summary or \"\", str(download_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e658ae83"
      },
      "source": [
        "### Launch Gradio Interface\n",
        "\n",
        "This section creates and launches the Gradio web interface. It allows you to record audio directly from your microphone or upload an audio file, and then process it through the defined function. You can also choose to generate a summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "c7858174",
        "outputId": "2c782f21-290e-4ec1-a788-ef17381d6afc"
      },
      "source": [
        "iface = gr.Interface(\n",
        "    fn=transcribe_and_summarize_or_translate,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Record or Upload Audio\"),\n",
        "        gr.Checkbox(label=\"Summarize Transcript?\", value=False)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcript\"),\n",
        "        gr.Textbox(label=\"Summary\"),\n",
        "        gr.File(label=\"Download Transcript\")\n",
        "    ],\n",
        "    title=\"WhisperNotes APP\",\n",
        "    description=\"Transcribe speech with Whisper; optionally summarize using any LLM via OpenRouter\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://055045d30f37764314.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://055045d30f37764314.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}